{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "084a5836-46c8-47ac-a1e0-e8348a525150",
      "cell_type": "markdown",
      "source": "# ProgrammingAssignment06_clusterAnalysis ",
      "metadata": {}
    },
    {
      "id": "c7bf8149-9aa3-4627-8f2c-49e531f9c5e9",
      "cell_type": "markdown",
      "source": "## 1. k-means using scikit-learn\nThe healthy_lifestyle dataset contains information on lifestyle measures such as amount of sunshine, pollution, and happiness levels for 44 major cities around the world. Apply k-means clustering to the cities' number of hours of sunshine and happiness levels.\n\n- Import the needed packages for clustering.\n- Initialize and fit a k-means clustering model using sklearn's Kmeans() function.\n- Use the user-defined number of clusters, init='random', n_init=10, random_state=123, and algorithm='elkan'.\n- Find the cluster centroids and inertia.\n\nEx: If the input is: 4\n\nthe output should be:\n\n- Centroids: [[ 0.8294  0.2562]\n [ 1.3106 -1.887 ]\n [-0.9471  0.8281]\n [-0.6372 -0.7943]]\n- Inertia: 16.4991",
      "metadata": {}
    },
    {
      "id": "ad54189b-81b1-4a58-9dd8-42ca5ba05310",
      "cell_type": "code",
      "source": "# Import needed packages\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nimport numpy as np\n\nhealthy = pd.read_csv('healthy_lifestyle.csv')\n\n# Input the number of clusters\nnumber = int(input())\n\n# Define input features\nX = healthy[['sunshine_hours', 'happiness_levels']]\n\n# Use StandardScaler() to standardize input features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX = pd.DataFrame(X, columns=['sunshine_hours', 'happiness_levels'])\nX = X.dropna()\n\n# Initialize a k-means clustering algorithm with a user-defined number of clusters, init='random', n_init=10,\n# random_state=123, and algorithm='elkan'\nkmeans = KMeans(n_clusters=number, init='random', n_init=10, random_state=123, algorithm='elkan')\n\n# Fit the algorithm to the input features\nkmeans.fit(X)\n\n# Find and print the cluster centroids\ncentroid = kmeans.cluster_centers_\nprint(\"Centroids:\", np.round(centroid, 4))\n\n# Find and print the cluster inertia\ninertia = kmeans.inertia_\nprint(\"Inertia:\", np.round(inertia, 4))",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "18700cd6-8069-458a-8822-61f755fd893e",
      "cell_type": "markdown",
      "source": "## 2. Hierarchical clustering using scikit-learn\nThe healthy_lifestyle dataset contains information on lifestyle measures such as amount of sunshine, pollution, and happiness levels for 44 major cities around the world. Apply agglomerative clustering to the cities' number of hours of sunshine and happiness levels using both sklearn and SciPy.\n\n- Import the needed packages for agglomerative clustering from sklearn and SciPy.\n- Initialize and fit an agglomerative clustering model using sklearn's AgglomerativeClustering() function. Use the user-defined number of clusters and ward linkage.\n- Add cluster labels to the input feature dataframe.\n- Calculate the distances between all instances using SciPy's pdist() function.\n- Convert the distance matrix to a square matrix using SciPy's squareform() function.\n- Define a clustering model with ward linkage using SciPy's linkage() function.\n\nEx: If the input is: 4\n\nthe output should be:\n|       | sunshine_hours | happiness_levels | labels |\n|-------|----------------|------------------|--------|\n| 0     | -0.691660      | 1.025642         | 3      |\n| 1     | 0.695725       | 0.801124         | 0      |\n| 2     | -0.645295      | 0.872562         | 3      |\n| 3     | -0.757641      | 0.933794         | 3      |\n| 4     | -1.098246      | 1.229750         | 3      |\n\n\nFirst five rows of the linkage matrix from SciPy:\n    \n - [[39. 40.  0.  2.]\n [28. 43.  0.  3.]\n [ 7. 18.  0.  2.]\n [ 8. 42.  0.  2.]\n [ 0.  3.  0.  2.]]",
      "metadata": {}
    },
    {
      "id": "f58d1471-d33e-43b5-839c-ba86c9509ebc",
      "cell_type": "code",
      "source": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.cluster.hierarchy import linkage, fcluster\n\n# Silence warning\nimport warnings\nwarnings.filterwarnings('ignore')\n\nhealthy = pd.read_csv('healthy_lifestyle.csv')\n\n# Input the number of clusters\nnumber = int(input())\n\n# Define input features\nX = healthy[['sunshine_hours', 'happiness_levels']]\n\n# Use StandardScaler() to standardize input features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX = pd.DataFrame(X, columns=['sunshine_hours', 'happiness_levels'])\nX = X.dropna()\n\n# Initialize and fit an agglomerative clustering model using ward linkage in scikit-learn, with a user-defined number of clusters\nagglo_clustering = AgglomerativeClustering(n_clusters=number, linkage='ward')\nX['labels'] = agglo_clustering.fit_predict(X)\nprint(X.head())\n\n# Perform agglomerative clustering using SciPy\n\n# Calculate the distances between all instances\ndistances = pdist(X[['sunshine_hours', 'happiness_levels']])\n\n# Convert the distance matrix to a square matrix\ndistance_matrix = squareform(distances)\n\n# Define a clustering model with ward linkage using SciPy's linkage() function\nclustersHealthyScipy = linkage(distances, method='ward')\n\n# Assign cluster labels from SciPy's agglomerative clustering to the data\nX['scipy_labels'] = fcluster(clustersHealthyScipy, t=number, criterion='maxclust')\n\nprint('First five rows of the linkage matrix from SciPy:\\n', np.round(clustersHealthyScipy[:5, :], 0))\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "78db51dd-7ad3-4f58-b0ff-ae71a6c306e8",
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d92b04d3-05bd-470c-a199-327a6b55679c",
      "cell_type": "markdown",
      "source": "## 3. DBSCAN using scikit-learn\n- Increase the **number of points sampled to 500**.\n- Apply the DBSCAN model with **epsilon=1** and **min_samples=8** to identify the number of core-points and outliers (or noise). \n- EX: if the epsilon=1 and min_samples = 10 and number of points sampled to 100.\n  - the number of core-points = 85\n  - the number of outliers    = 11",
      "metadata": {}
    },
    {
      "id": "fdeae7b7-d4e7-483a-81bc-1d4bd4d6716c",
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import DBSCAN\n\n# Load the full grocery customer dataset and take a random sample of 500 instances\ndata = pd.read_csv('customer_personality.csv').sample(500, random_state=123)\n\n# Use StandardScaler() to standardize input features\nX = data[['Fruits', 'Meats']]\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX = pd.DataFrame(X)\n\n# Apply DBSCAN with epsilon=1 and min_samples=8\ndbscan = DBSCAN(eps=1, min_samples=8)\ndbscan = dbscan.fit(X)\n\n# Print the cluster labels and core point indices\nprint('Labels:', dbscan.labels_)\nprint('Core points:', dbscan.core_sample_indices_)\nprint('Number of core points:', len(dbscan.core_sample_indices_))\n\n# Calculate the number of outliers (noise points labeled as -1)\nnum_outliers = sum(dbscan.labels_ == -1)\nprint('Number of outliers:', num_outliers)\n\n# Add the cluster labels to the dataset as strings\ndata['clusters'] = dbscan.labels_.astype(str)\n\n# Sort by cluster label (for plotting purposes)\ndata.sort_values(by='clusters', inplace=True)\n\n# Plot clusters on the original data\np = sns.scatterplot(data=data, x='Fruits',\n                    y='Meats', hue='clusters',\n                    style='clusters')\np.set_xlabel('Fruits', fontsize=16)\np.set_ylabel('Meats', fontsize=16)\np.legend(title='DBSCAN')\nplt.show()\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3ea545b6-c934-42ca-9f5a-be7bf69d22d9",
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    }
  ]
}